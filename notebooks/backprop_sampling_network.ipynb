{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BackProp Sampling Network\n",
    "\n",
    "This is a continuation of the [Sampling Neural Network](sampling_neural_network.ipynb) notebook.  We noted there that our training process was extremely inefficient.  \n",
    "\n",
    "Recall that we want to Gibbs-sample our weight values based on:\n",
    "$$\n",
    "W_{\\alpha} \\sim Categorical \\Big(softmax\\big(\\big[log(p(W_{\\alpha}=c_k))+\\sum_{n=1}^{N_{samples}}log(f(X_n,W_{w_{\\alpha}=c_k}))_{Y_n},k \\in 1..K\\big]\\big)\\Big)\n",
    "$$\n",
    "\n",
    "The problem was that we had to recompute the term \n",
    "$$\n",
    "g(x, w_{w_{\\alpha}=c_k}) \\equiv logL(Y|X,W) = \\sum_{n=1}^{N_{samples}}log(f(x_n,w_{w_{\\alpha}=c_k}))_{y_n}\n",
    "$$\n",
    ", which corresponds to a full forward pass over all data points, for every $\\alpha$ for every possible parameter value $c_k$ for every sampling pass through the parameters.\n",
    "\n",
    "Instead, we can make the same apporoximation as in BackPropagation - that is, $g(x, w)$ is locally linear in w.\n",
    "\n",
    "So \n",
    "$$\n",
    "g(x,w_{w_{\\alpha}=c_k}) \\simeq g(x, w) + \\frac{\\partial g(x, w)}{\\partial w_{\\alpha}} (c_k - w_\\alpha)\n",
    "$$\n",
    "\n",
    "Since we're doing a softmax on the result (over k), a constant shift (first term) does not matter, so we just compute:\n",
    "\n",
    "$$\n",
    "W_{\\alpha} \\sim Categorical \\Big(softmax\\big(\\big[log(p(W_{\\alpha}=c_k))+\\frac{\\partial g(x, w)}{\\partial w_{\\alpha}} (c_k - w_\\alpha),k \\in 1..K\\big]\\big)\\Big)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment\n",
    "\n",
    "We can run this and compare the results on a Multi-layer perceptron.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "which_dataset = 'mnist'    # 'mnist' or 'clusters'\n",
    "n_hidden = 100\n",
    "mlp_eta = 0.1\n",
    "gibbs_frac_update = 0.01\n",
    "possible_ws = (-1, 0, 1)\n",
    "n_epochs = 10\n",
    "n_test_points = 20\n",
    "minibatch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from experimental.sampling_mlp import GibbsSamplingMLP\n",
    "from plato.tools.cost import negative_log_likelihood\n",
    "from plato.tools.networks import MultiLayerPerceptron\n",
    "from plato.tools.online_prediction.online_predictors import GradientBasedPredictor\n",
    "from plato.tools.optimizers import SimpleGradientDescent\n",
    "from utils.benchmarks.predictor_comparison import compare_predictors\n",
    "from utils.datasets.mnist import get_mnist_dataset\n",
    "from utils.datasets.synthetic_clusters import get_synthetic_clusters_dataset\n",
    "from utils.tools.mymath import sqrtspace\n",
    "from general.should_be_builtins import bad_value\n",
    "import numpy as np\n",
    "\n",
    "dataset = \\\n",
    "    get_synthetic_clusters_dataset(n_dims=100) if which_dataset == 'clusters' else \\\n",
    "    get_mnist_dataset(flat = True) if which_dataset == 'mnist' else \\\n",
    "    bad_value(which_dataset, 'No dataset named \"%s\"' % which_dataset)\n",
    "\n",
    "results = compare_predictors(\n",
    "    dataset = dataset,\n",
    "    online_predictors={\n",
    "        'MLP': GradientBasedPredictor(\n",
    "            function = MultiLayerPerceptron(layer_sizes = [n_hidden, dataset.n_categories], input_size = dataset.input_shape[0], output_activation='softmax', w_init = lambda n_in, n_out: 0.1*np.random.randn(n_in, n_out)),\n",
    "            cost_function=negative_log_likelihood,\n",
    "            optimizer=SimpleGradientDescent(eta = mlp_eta),\n",
    "            ).compile(),\n",
    "        'Gibbs-MLP': GibbsSamplingMLP(\n",
    "            layer_sizes = [n_hidden, dataset.n_categories],\n",
    "            input_size = dataset.input_shape[0],\n",
    "            possible_ws = possible_ws,\n",
    "            frac_to_update = gibbs_frac_update,\n",
    "            output_activation='softmax'\n",
    "            ).compile(mode = 'tr'),\n",
    "        },\n",
    "    evaluation_function='percent_argmax_correct',\n",
    "    minibatch_size=minibatch_size,\n",
    "    accumulators={\n",
    "        'MLP': None,\n",
    "        'Gibbs-MLP': 'avg',\n",
    "        },\n",
    "    test_epochs=sqrtspace(0, n_epochs, n_test_points)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils.benchmarks.plot_learning_curves import plot_learning_curves\n",
    "from plotting.notebook_plots import link_and_show\n",
    "plot_learning_curves(results)\n",
    "link_and_show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}